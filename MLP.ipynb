{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "A multi-layer perceptron (MLP) is a type of artificial neural network consisting of multiple layers of neurons. The neurons in the MLP typically use nonlinear activation functions, allowing the network to learn complex patterns in data. MLPs are significant in machine learning because they can learn nonlinear relationships in data, making them powerful models for tasks such as classification, regression, and pattern recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/60], Loss: 0.6685\n",
      "Epoch [2/60], Loss: 0.5884\n",
      "Epoch [3/60], Loss: 0.5270\n",
      "Epoch [4/60], Loss: 0.5100\n",
      "Epoch [5/60], Loss: 0.5053\n",
      "Epoch [6/60], Loss: 0.4985\n",
      "Epoch [7/60], Loss: 0.4950\n",
      "Epoch [8/60], Loss: 0.4900\n",
      "Epoch [9/60], Loss: 0.4877\n",
      "Epoch [10/60], Loss: 0.4817\n",
      "Epoch [11/60], Loss: 0.4805\n",
      "Epoch [12/60], Loss: 0.4763\n",
      "Epoch [13/60], Loss: 0.4739\n",
      "Epoch [14/60], Loss: 0.4718\n",
      "Epoch [15/60], Loss: 0.4695\n",
      "Epoch [16/60], Loss: 0.4654\n",
      "Epoch [17/60], Loss: 0.4639\n",
      "Epoch [18/60], Loss: 0.4641\n",
      "Epoch [19/60], Loss: 0.4600\n",
      "Epoch [20/60], Loss: 0.4591\n",
      "Epoch [21/60], Loss: 0.4551\n",
      "Epoch [22/60], Loss: 0.4538\n",
      "Epoch [23/60], Loss: 0.4493\n",
      "Epoch [24/60], Loss: 0.4488\n",
      "Epoch [25/60], Loss: 0.4468\n",
      "Epoch [26/60], Loss: 0.4459\n",
      "Epoch [27/60], Loss: 0.4407\n",
      "Epoch [28/60], Loss: 0.4427\n",
      "Epoch [29/60], Loss: 0.4378\n",
      "Epoch [30/60], Loss: 0.4341\n",
      "Epoch [31/60], Loss: 0.4316\n",
      "Epoch [32/60], Loss: 0.4311\n",
      "Epoch [33/60], Loss: 0.4298\n",
      "Epoch [34/60], Loss: 0.4269\n",
      "Epoch [35/60], Loss: 0.4242\n",
      "Epoch [36/60], Loss: 0.4202\n",
      "Epoch [37/60], Loss: 0.4196\n",
      "Epoch [38/60], Loss: 0.4154\n",
      "Epoch [39/60], Loss: 0.4124\n",
      "Epoch [40/60], Loss: 0.4115\n",
      "Epoch [41/60], Loss: 0.4099\n",
      "Epoch [42/60], Loss: 0.4065\n",
      "Epoch [43/60], Loss: 0.4031\n",
      "Epoch [44/60], Loss: 0.3984\n",
      "Epoch [45/60], Loss: 0.3974\n",
      "Epoch [46/60], Loss: 0.3944\n",
      "Epoch [47/60], Loss: 0.3898\n",
      "Epoch [48/60], Loss: 0.3918\n",
      "Epoch [49/60], Loss: 0.3869\n",
      "Epoch [50/60], Loss: 0.3821\n",
      "Epoch [51/60], Loss: 0.3822\n",
      "Epoch [52/60], Loss: 0.3797\n",
      "Epoch [53/60], Loss: 0.3731\n",
      "Epoch [54/60], Loss: 0.3731\n",
      "Epoch [55/60], Loss: 0.3722\n",
      "Epoch [56/60], Loss: 0.3702\n",
      "Epoch [57/60], Loss: 0.3625\n",
      "Epoch [58/60], Loss: 0.3610\n",
      "Epoch [59/60], Loss: 0.3626\n",
      "Epoch [60/60], Loss: 0.3618\n",
      "Accuracy on test data: 0.7562\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "data = pd.read_csv(url, sep=';')\n",
    "\n",
    "X = data.drop('quality', axis=1).values\n",
    "y = data['quality'].values\n",
    "\n",
    "y = np.where(y >= 6, 1, 0)  # 1 - good one , 0 - bad one\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# transforming to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# data-loader creation for mini-batches\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "class WineQualityMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WineQualityMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(11, 64)  # input layer (11 features → 64 neurons)\n",
    "        self.fc2 = nn.Linear(64, 32)  # hidden layer (64 → 32)\n",
    "        self.fc3 = nn.Linear(32, 1)   # output layer (32 → 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()   # sigmoid due to binarity\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = WineQualityMLP()\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.BCELoss()  # binary cross-entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 60\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # switch to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()  # Обнуляем градиенты\n",
    "        outputs = model(X_batch)  # forward porpagation\n",
    "        loss = criterion(outputs, y_batch)  # calculate loss\n",
    "        loss.backward()  # error backpropagation\n",
    "        optimizer.step()  # new weights\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "model.eval()  # evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        predicted = (outputs > 0.5).float()  # classification\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on test data: {accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
